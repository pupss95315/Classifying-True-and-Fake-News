{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "true_csv = pd.read_csv('./dataset/True.csv')\n",
    "fake_csv = pd.read_csv('./dataset/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21417\n"
     ]
    }
   ],
   "source": [
    "N_true = len(true_csv)\n",
    "print(N_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23481\n"
     ]
    }
   ],
   "source": [
    "N_fake = len(fake_csv)\n",
    "print(N_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubset(ind, L, leng): # L: dataframe\n",
    "    cnt = 0\n",
    "    K = [[] for i in range(leng)]\n",
    "    for i in range(len(L)): \n",
    "        if i in ind:\n",
    "            K[cnt] = L[i]\n",
    "            cnt = cnt + 1\n",
    "    return K;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rand_N = random.randint(1000, 5000) # total size of test data\n",
    "rand_true = random.randint(int(rand_N * 0.4), int(rand_N * 0.6)) # size of true\n",
    "rand_fake = rand_N - rand_true # size of fake\n",
    "ind_true_test = random.sample(list(range(N_true)), rand_true) # indices of true test\n",
    "ind_fake_test = random.sample(list(range(N_fake)), rand_fake) # indices of fake test\n",
    "ind_true_train = list(set(list(range(N_true))) - set(ind_true_test)) # indices of true train\n",
    "ind_fake_train = list(set(list(range(N_fake))) - set(ind_fake_test)) # indices of fake train\n",
    "\n",
    "true_train = true_csv[true_csv.index.isin(ind_true_train)]['text']\n",
    "fake_train = fake_csv[fake_csv.index.isin(ind_fake_train)]['text']\n",
    "true_test = true_csv[true_csv.index.isin(ind_true_test)]['text']\n",
    "fake_test = true_csv[true_csv.index.isin(ind_fake_test)]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "R = [[], [], [], []]\n",
    "\n",
    "for i in range(2000):\n",
    "    R[0].append(true_csv.iloc[i])\n",
    "    R[1].append(fake_csv.iloc[i])\n",
    "    \n",
    "for i in range(500):\n",
    "    R[2].append(true_csv.iloc[i])\n",
    "    R[3].append(fake_csv.iloc[i])\n",
    "    \n",
    "# for i in range(4):\n",
    "#     print(len(R[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    R[0][i] = R[0][i][1]\n",
    "    R[1][i] = R[1][i][1]\n",
    "    \n",
    "for i in range(500):\n",
    "    R[2][i] = R[2][i][1]\n",
    "    R[3][i] = R[3][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc = []\n",
    "for i in range(2000):\n",
    "    all_doc.append(R[0][i])\n",
    "for i in range(2000):\n",
    "    all_doc.append(R[1][i])\n",
    "for i in range(500):\n",
    "    all_doc.append(R[2][i])\n",
    "for i in range(500):\n",
    "    all_doc.append(R[3][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    WASHINGTON (Reuters) - The head of a conservat...\n",
       "1    WASHINGTON (Reuters) - Transgender people will...\n",
       "2    WASHINGTON (Reuters) - The special counsel inv...\n",
       "4    SEATTLE/WASHINGTON (Reuters) - President Donal...\n",
       "5    WEST PALM BEACH, Fla./WASHINGTON (Reuters) - T...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_list = true_train.tolist()\n",
    "fake_list = fake_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# true =  18872\n",
      "# fake =  21503\n"
     ]
    }
   ],
   "source": [
    "N_true_train = len(true_list)\n",
    "print(\"# true = \", N_true_train)\n",
    "N_fake_train = len(fake_list)\n",
    "print(\"# fake = \", N_fake_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_split(L): # L: list\n",
    "    n = len(L)\n",
    "    \n",
    "    # lower\n",
    "    for i in range(n):\n",
    "        L[i] = L[i].lower()\n",
    "    \n",
    "    # split\n",
    "    punctuation_list = [\".,!?\\'\\\"-{}[]():;`@#$%^&*~+=/|<>_\"]\n",
    "    token_list = [[] for i in range(n)]\n",
    "    for i in range(n):\n",
    "        for char in punctuation_list:\n",
    "            L[i] = L[i].replace(char, \" \")\n",
    "        token_list[i] = L[i].split()\n",
    "        k = len(token_list[i])\n",
    "        for j in range(k):\n",
    "            if len(token_list[i][k - j - 1]) <= 3 or token_list[i][k - j - 1].isalpha() == False:\n",
    "                token_list[i].pop(k - j - 1)\n",
    "        \n",
    "    return token_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_token_list = lower_split(true_list)\n",
    "f_token_list = lower_split(fake_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "swFile = open('stoplist.txt', 'r')\n",
    "stopWords = swFile.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = list(set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "                  'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', \n",
    "                  'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "                  'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', \n",
    "                  'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
    "                  'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "                  'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', \n",
    "                  'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \n",
    "                  'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn',\n",
    "                  \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "                  \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]))\n",
    "stopwords_list = set(stopWords + stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(L, stopwords_list):\n",
    "    n = len(L)\n",
    "    stop_list = []\n",
    "\n",
    "    for i in range(n):\n",
    "        temp_list = []\n",
    "        for j in range(len(L[i])):\n",
    "            if(L[i][j] not in stopwords_list):\n",
    "                temp_list.append(L[i][j])\n",
    "        stop_list.append(temp_list)\n",
    "    \n",
    "    return stop_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stop_list = stopwords(t_token_list, stopwords_list)\n",
    "f_stop_list = stopwords(f_token_list, stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(L):\n",
    "    n = len(L)\n",
    "    S = [[] for i in range(n)]\n",
    "    \n",
    "    for i in (range(n)):\n",
    "        S[i] = [ps.stem(L[i][j]) for j in range(len(L[i]))]\n",
    "\n",
    "    return S;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list_of_list = stem(t_stop_list)\n",
    "f_list_of_list = stem(f_stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = stopwords(t_list_of_list, stopwords_list)\n",
    "f_train = stopwords(f_list_of_list, stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [] # true and fake mixed together\n",
    "train.extend(t_train)\n",
    "train.extend(f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Labels(D, N_true_train) :\n",
    "    labels = [[] for i in range(len(D))]\n",
    "    for i in range(len(D)):\n",
    "        if i < N_true_train:\n",
    "            labels[i] = 1\n",
    "        else:\n",
    "            labels[i] = 0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Top k Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatLargestVal(L, k):\n",
    "    L = L[L.index < k]\n",
    "    return L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contingency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t = N_true_train + N_fake_train\n",
    "n_c = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Contingency(df_all, df_class, n_d): # df of a term in collection/ in the class/ docs num in class\n",
    "    cont = np.zeros((3, 3))\n",
    "    '''\n",
    "    cont:             a term t\n",
    "                present      absent      total\n",
    "    on-topic    [0][0]       [0][1]       n_d\n",
    "    off-topic   [1][0]       [1][1]     n_t - n_d\n",
    "    total         S          n_t - S      n_t\n",
    "    '''\n",
    "    cont[0][2] = n_d\n",
    "    cont[0][0] = df_class\n",
    "    cont[0][1] = cont[0][2] - cont[0][0]\n",
    "    \n",
    "    cont[1][2] = n_t - n_d \n",
    "    cont[1][0] = df_all - cont[0][0]\n",
    "    cont[1][1] = cont[1][2] - cont[1][0]\n",
    "    \n",
    "    cont[2][0] = df_all # df_all\n",
    "    cont[2][1] = cont[0][1] + cont[1][1]\n",
    "    cont[2][2] = cont[0][2] + cont[1][2]\n",
    "    \n",
    "    return cont;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods of Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Chi-square Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChiSquare(df_all, df_class, n_d): # df of D / df of class C / docs num in class\n",
    "    N = Contingency(df_all, df_class, n_d)\n",
    "    E = np.zeros((2, 2))\n",
    "    '''\n",
    "    cont:             a term t\n",
    "                present      absent      total\n",
    "    on-topic    [0][0]       [0][1]     n_d = \n",
    "    off-topic   [1][0]       [1][1]     n_t - n_d\n",
    "    total         S          n_t - S       n_t\n",
    "    '''\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            E[i][j] = n_t * (N[i][2] / n_t) * (N[2][j] / n_t)\n",
    "    \n",
    "    chi = float(0)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            chi += (N[i][j] - E[i][j]) ** 2 / E[i][j]\n",
    "    \n",
    "    return chi;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Expected Mutual Information Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMI(df_all, df_class, n_d): # df of D / df of class C / docs num in class\n",
    "    N = Contingency(df_all, df_class, n_d)\n",
    "    I = float(0)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            # smoothing!\n",
    "            P_tc = (N[i][j] + 0.5) / (N[2][2] + 2)\n",
    "            P_t = (N[2][j] + 1) / (N[2][2] + 2)\n",
    "            P_c = (N[i][2] + 1) / (N[2][2] + 2)\n",
    "            I = I + P_tc * np.log(P_tc / (P_t * P_c))\n",
    "    \n",
    "    return I;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Log Likilihood Rate Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLR(df_all, df_class, n_d): # df of D / df of class C / docs num in class\n",
    "    N = Contingency(df_all, df_class, n_d)\n",
    "    E = np.zeros((2, 2))\n",
    "    '''\n",
    "    cont:             a term t\n",
    "                present      absent      total\n",
    "    on-topic    [0][0]       [0][1]       n_d\n",
    "    off-topic   [1][0]       [1][1]     n_t - n_d\n",
    "    total         S          n_t - S      n_t\n",
    "    '''\n",
    "    N00 = N[1][1]\n",
    "    N01 = N[0][1]\n",
    "    N10 = N[1][0]\n",
    "    N11 = N[0][0]\n",
    "    N = N11+N10+N01+N00\n",
    "    \n",
    "    likelihood = float(0)\n",
    "    likelihood = -2 * np.log ((((N11+N01)/N)**N11 * (1- (N11+N01)/(N))**N10 * ((N11+N01)/N)**N01 * (1- (N11+N01)/(N))**N00) / ((N11/(N11+N10))**N11 * (1-N11/(N11+N10))**N10 * (N01/(N01+N00))**N01 * (1-N01/(N01+N00))**N00))\n",
    "    \n",
    "    return likelihood;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocFreqInClass(V, c): # V: train_df/ c = -1: for whole collection, else: c = 1 -> true, c = 0 -> false\n",
    "    if c == 1:\n",
    "        txt_C = V[V['label'] == 1]['terms'].tolist()\n",
    "    elif c == 0:\n",
    "        txt_C = V[V['label'] == 0]['terms'].tolist()\n",
    "    else:\n",
    "        txt_C = V['terms'].tolist()\n",
    "    n = len(txt_C)\n",
    "    termsUniq = []\n",
    "\n",
    "    # make words unique in each document\n",
    "    for i in (range(n)):\n",
    "        tUniq = list(set(txt_C[i]))\n",
    "        termsUniq.extend(tUniq)\n",
    "    \n",
    "    # compute df for the whole collection\n",
    "    \n",
    "    termsDF = pd.DataFrame(pd.Series(termsUniq).value_counts().to_frame('df')).reset_index()\n",
    "    termsDF.columns = ['term', 'df']\n",
    "    termsDF = termsDF.sort_values(by = 'df', ascending = False).reset_index().drop(columns = ['index'])\n",
    "    return termsDF; # a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectFeatures(V, c, k, typ):\n",
    "    # V: train_df / c: int (class)/ k: int (# of features)/ typ: int (type of selection)\n",
    "    # c: 1 -> true, 0 -> fake\n",
    "    # typ: 1 -> chi, 2 -> EMI, 3 -> LLR # single\n",
    "    V_uniq = DocFreqInClass(V, -1) # unique terms in the whole training set\n",
    "    txt_C = DocFreqInClass(V, c) # unique terms in class c\n",
    "    n_v = len(txt_C) # number of unique terms in class c\n",
    "    n_d = len(V[V['label'] == c])\n",
    "    term = txt_C['term'].tolist()\n",
    "    df_class = txt_C['df'].tolist()\n",
    "    df_all = V_uniq['df'][V_uniq['term'].isin(txt_C['term'])].tolist()\n",
    "    A = np.zeros(n_v)\n",
    "    cArr = [c for i in range(k)]\n",
    "    ind = [i for i in range(k)]\n",
    "    L = pd.DataFrame()\n",
    "    if typ == 1:\n",
    "        for t in range(n_v):\n",
    "            A[t] = ChiSquare(df_all[t], df_class[t], n_d) # n_d?\n",
    "        L_cand = pd.DataFrame({'term': term, 'chi-sq': A}).sort_values(by = 'chi-sq', ascending = False).reset_index().drop(columns = ['index', 'chi-sq'])\n",
    "        L = FeatLargestVal(L_cand, k);\n",
    "    elif typ == 2:\n",
    "        for t in range(n_v):\n",
    "            A[t] = EMI(df_all[t], df_class[t], n_d)\n",
    "        L_cand = pd.DataFrame({'term': term, 'EMI': A}).sort_values(by = 'EMI', ascending = False).reset_index().drop(columns = ['index', 'EMI'])\n",
    "        L = FeatLargestVal(L_cand, k);\n",
    "    elif typ == 3:\n",
    "        for t in range(n_v):\n",
    "            A[t] = LLR(df_all[t], df_class[t], n_d)\n",
    "        L_cand = pd.DataFrame({'term': term, 'LLR': A}).sort_values(by = 'LLR', ascending = False).reset_index().drop(columns = ['index', 'LLR'])\n",
    "        L = FeatLargestVal(L_cand, k);\n",
    "    \n",
    "    L.insert(1, 'class', cArr, True)\n",
    "    return L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def SelectDoubleFeatures(V, c, k, typ): # typ: 1 -> chi, 2 -> EMI, 3 -> LLR\n",
    "    V_uniq = DocFreqInClass(V, -1) # unique terms in the whole training set\n",
    "    txt_C = DocFreqInClass(V, c) # unique terms in class c\n",
    "    n_v = len(txt_C)\n",
    "    term = txt_C['term']\n",
    "    A = np.zeros((2, n_v)) # 2d: chi-square, EMI\n",
    "    cArr = [c for i in range(k)]\n",
    "    ind = [i for i in range(k)]\n",
    "    for t in range(n_v):\n",
    "        A[0][t] = ChiSquare(V_uniq, txt_C, term[t]) # A[0]: chi-sq\n",
    "        A[1][t] = EMI(V_uniq, txt_C, term[t]) # A[1]: EMI\n",
    "    \n",
    "    L_cand = [[] for i in range(2)] # chi-sq, EMI\n",
    "    L_cand[0] = pd.DataFrame({'term': term, 'chi-sq': A[0]}).sort_values(by = 'chi-sq', ascending = False).reset_index().drop(columns = ['index'])\n",
    "    L_cand[1] = pd.DataFrame({'term': term, 'EMI': A[1]}).sort_values(by = 'EMI', ascending = False).reset_index().drop(columns = ['index'])\n",
    "    \n",
    "    L_top = pd.DataFrame()\n",
    "    temp = [[] for i in range(2)]\n",
    "    for i in range(2):\n",
    "        temp[i] = L_cand[i].loc[:int(n_v * 0.2)] # top 20%\n",
    "    L_top = pd.merge(temp[0], temp[1], how = 'outer', on = ['term']).sort_values(by = 'chi-sq', ascending = False).reset_index().drop(columns = ['index'])\n",
    "    # union\n",
    "    \n",
    "    L_last = pd.DataFrame()\n",
    "    for i in range(2):\n",
    "        temp[i] = L_cand[i].loc[int(n_v * 0.2):]\n",
    "    L_last = pd.merge(temp[0], temp[1], how = 'inner', on = ['term']).sort_values(by = 'EMI', ascending = False).reset_index().drop(columns = ['index'])\n",
    "    # intersection\n",
    "    \n",
    "    L = pd.DataFrame()\n",
    "    L_top = L_top.drop(columns = ['chi-sq', 'EMI'])\n",
    "    if len(L_top) >= k: # L_top >= k terms\n",
    "        L = FeatLargestVal(L_top, k)\n",
    "    else:\n",
    "        L_last = L_last.drop(columns = ['chi-sq', 'EMI'])\n",
    "        L = pd.merge(L_top, L_last, how = 'outer', on = ['term']).reset_index().drop(columns = ['index'])\n",
    "        L = FeatLargestVal(L, k)\n",
    "\n",
    "    L['class'] = cArr\n",
    "    return L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureSelection(train_df, k, typ) :\n",
    "    final = [[] for i in range(2)] # a list of dataframe\n",
    "    for i in range(2):\n",
    "        L = SelectFeatures(train_df, i, k, typ)\n",
    "        final[i] = L\n",
    "    final_all = pd.DataFrame()\n",
    "    for i in range(2):\n",
    "        k_1 = (k/2)-1\n",
    "        final_all = final_all.append(final[i].loc[:k_1])\n",
    "    final_all = final_all.reset_index().drop(columns = ['index'])\n",
    "    ind = [i for i in range(k)]\n",
    "    final_all.insert(2, 'i', ind, True)\n",
    "    return final_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent Documents as Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocToVector(D, f, is_training): #D: list of doc/ f: final feature/ is_training: bool\n",
    "    #temp = []\n",
    "    #if is_training == True:\n",
    "        #temp = train\n",
    "    #else:\n",
    "        #temp = test\n",
    "        \n",
    "    df = []\n",
    "    for i in range(len(f)):\n",
    "        term = f.at[i,'term']\n",
    "        CNT = 0\n",
    "        for j in range(len(D)):\n",
    "            if term in D[j]:\n",
    "                CNT += 1\n",
    "        df.append(CNT)\n",
    "    \n",
    "    vecDoc = [[] for i in range(len(D))]\n",
    "    for i in range(len(D)):\n",
    "        vecDoc[i] = [float(0) for j in range(len(f))]\n",
    "        for j in range(len(f)):\n",
    "            tf = D[i].count(f.at[j,'term'])\n",
    "            ### smoothing? df = 0, division by zero\n",
    "            if df[j] != 0:\n",
    "                tfIdf = tf * np.log(len(D) / df[j])\n",
    "            else:\n",
    "                tfIdf = tf * np.log(len(D)) # df = 1\n",
    "            vecDoc[i][j] = tfIdf\n",
    "        # normalize\n",
    "        v = np.array(vecDoc[i])\n",
    "        l = float(1)\n",
    "        if np.sum(v) != 0:\n",
    "            l = np.sqrt(np.sum(v ** 2))\n",
    "        vecDoc[i] = v * (1/l)\n",
    "    return(vecDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeNN(D, k, d):\n",
    "    # D: list of numpy TF-IDF unit vectors of each training doc (dimension = # of features)\n",
    "    # d: numpy TF-IDF unit vector of a testing doc (dimension = # of features)\n",
    "    n = len(D)\n",
    "    vecs = D['vec'].tolist()\n",
    "    # assume vec and d are normalized\n",
    "    cos = np.array([np.dot(vecs[i], d) for i in range(n)])\n",
    "    ind = [i for i in range(n)]\n",
    "    df = pd.DataFrame({'ind': ind, 'cos': cos}).sort_values(by = 'cos', ascending = False).reset_index().drop(columns = ['index'])\n",
    "    kNN = FeatLargestVal(df, k)['ind'].tolist()\n",
    "    return kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArgMaxScore(score): # np array of scores\n",
    "    m = np.amax(score)\n",
    "    argmax = list(score).index(m) # index\n",
    "    return argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyKNN(D, k, d): # k = 3 or 5: common\n",
    "    # D: dataframe of TF-IDF vectors + labels of training docs/ k: parameter for kNN/ d: TF-IDF vector of a doc\n",
    "    Sk = ComputeNN(D, k, d) # list of k neighbors (index)\n",
    "    p = np.zeros(2)\n",
    "    for i in range(len(Sk)):\n",
    "        if D.at[Sk[i], 'label'] == 1: # true\n",
    "            p[1] = p[1] + 1\n",
    "        else: # fake\n",
    "            p[0] = p[0] + 1 \n",
    "    p = p * (1/k)\n",
    "    c = ArgMaxScore(p)\n",
    "    return c # class: 1 or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Naive Base Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tct(text_c, t) :\n",
    "    num = 0\n",
    "    for term in text_c :\n",
    "        if t == term :\n",
    "            num += 1\n",
    "    return num\n",
    "def concatenate_text_of_all_docs(D) :\n",
    "    text_c = []\n",
    "    for i in range(len(D)) :\n",
    "        text_c.extend(D[i])\n",
    "    return text_c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D : training documents\n",
    "# f : features\n",
    "def TrainNB(t_train, f_train, f, N_true_train, N_fake_train) :\n",
    "    prior = []\n",
    "    condprob = []\n",
    "    #print(N_fake_train)\n",
    "    #print(N_true_train)\n",
    "    for i in range(2) :\n",
    "        if i == 0 :\n",
    "            class_doc_num = N_fake_train\n",
    "            D = f_train\n",
    "            feature = f[\"term\"].loc[:399]\n",
    "        else :\n",
    "            class_doc_num = N_true_train\n",
    "            D = t_train\n",
    "            feature = f[\"term\"].loc[400:799]\n",
    "        prior.append(class_doc_num / (N_fake_train + N_true_train))\n",
    "        temp_tct = []\n",
    "        temp_con = []\n",
    "        total = 0\n",
    "        text_c = concatenate_text_of_all_docs(D)\n",
    "        for t in feature :\n",
    "            tct = count_tct(text_c, t)\n",
    "            temp_tct.append(tct)\n",
    "            total += tct\n",
    "        for t in feature :\n",
    "            tct = count_tct(text_c, t)\n",
    "            temp_con.append((tct+1) / (total + len(feature)))\n",
    "        d = {\"term\" : feature , \"condprob\" : temp_con}\n",
    "        condprob.append(pd.DataFrame(data = d))\n",
    "    return prior, condprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W : tokens in document\n",
    "# f : feature\n",
    "def ApplyNB(W, prior, condprob, f) :\n",
    "    score = []\n",
    "    \n",
    "    for i in range(2) :\n",
    "        if i == 0 :\n",
    "            feature = f[\"term\"].loc[:399]\n",
    "        else :\n",
    "            feature = f[\"term\"].loc[400:799]\n",
    "        cur = math.log10(prior[i])\n",
    "        for t in W :\n",
    "            if t in feature :\n",
    "                cur += math.log10(condprob[i][\"condprob\"].loc[condprob[i][\"term\"] == t])\n",
    "            else :\n",
    "                cur += math.log10(1/5000)\n",
    "        score.append(cur)\n",
    "    max_s = score[0]\n",
    "    c = 0\n",
    "    for i in range(len(score)) :\n",
    "        if score[i] > max_s :\n",
    "            max_s = score[i]\n",
    "            c = i\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prior, condprob = TrainNB(t_train, f_train, final_all, N_true, N_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = []\n",
    "# for i in range(len(test)) :\n",
    "#    ans.append(ApplyNB(test[i], prior, condprob, final_all))\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Rocchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainRocchio(D, doclen, N_true_train): #D: training doc, doclen: dimension of doc, index < N_true_train -> true\n",
    "    u = []\n",
    "    u.append([]) # for class true\n",
    "    u.append([]) # for class fake\n",
    "    for i in range(doclen): # i-dimension centroid\n",
    "        sum_true = 0\n",
    "        sum_fake = 0\n",
    "        for j in range(N_true_train):\n",
    "            sum_true += D[j][i]\n",
    "        for j in range(N_true_train, len(D)):\n",
    "            sum_fake += D[j][i]\n",
    "        u[1].append(sum_true / N_true_train)\n",
    "        u[0].append(sum_fake / (len(D) - N_true_train + 1))      \n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyRocchio(u, d): #u: centroid, d: a doc vector\n",
    "    distance = [] # len(distance) = len(u) = 2(true / fake)\n",
    "    for i in range(len(u)):\n",
    "        sum = 0\n",
    "        for j in range(len(d)): # j dimension\n",
    "            sum += (u[i][j] - d[j])**2 \n",
    "        distance.append(sum)\n",
    "    return(np.argmin(distance)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centroid = TrainRocchio(train_vec, len(final_all), N_true_train) # 1 for true, 0 for fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for true, 0 for fake\n",
    "# ans2 = []\n",
    "# for i in range(len(test_vec)):\n",
    "#    ans2.append(ApplyRocchio(centroid, test_vec[i]))\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train = train_df.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into 10 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I : document indicies\n",
    "# D : training doc\n",
    "# k : # of folds\n",
    "def SplitData(I, D, k) :\n",
    "    random.shuffle(I)\n",
    "    ind_fold = []\n",
    "    ind_fold.append([])\n",
    "    fold = []\n",
    "    fold.append([])\n",
    "    fold_amount = int(len(I) / k)\n",
    "    \n",
    "    for i in range(k) :\n",
    "        ind_cur = []\n",
    "        cur = []\n",
    "        for j in range(fold_amount) :\n",
    "            ind_cur.append(I[i*fold_amount + j])\n",
    "            cur.append(D[I[i*fold_amount + j]])\n",
    "        ind_fold[0].append(ind_cur)\n",
    "        fold[0].append(cur)\n",
    "\n",
    "    # evenly assign the remaining document into folds\n",
    "    remain = len(I) - fold_amount*k\n",
    "    for i in range(remain) :\n",
    "        ind_fold[0][i].append(I[fold_amount*k + i])\n",
    "        fold[0][i].append(D[I[fold_amount*k + i]])\n",
    "    return ind_fold[0], fold[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split True and Fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I : document indicies\n",
    "# D : training doc\n",
    "def SeperateTrueFake(I, D, N_true_train) :\n",
    "    new_t_train = []\n",
    "    new_f_train = []\n",
    "    labels = []\n",
    "    for i in range(len(D)) :\n",
    "        if I[i] < N_true_train :\n",
    "            new_t_train.append(D[i])\n",
    "            labels.append(1)\n",
    "        else :\n",
    "            new_f_train.append(D[i])\n",
    "            labels.append(0)\n",
    "    return new_t_train, new_f_train, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountF1(pre, recall) :\n",
    "    if (pre + recall) == 0 :\n",
    "        return 0\n",
    "    F1 = (2*pre*recall) / (pre + recall)\n",
    "    return F1\n",
    "\n",
    "# fake : class 0 \n",
    "# true : class 1 \n",
    "# df : dataframe with two attributes, doc Id and doc Value (predict class)\n",
    "def CountPreRecall(df, N_true_train) :\n",
    "    tp = 0\n",
    "    fn = 0 \n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    for i in range(len(df[\"Id\"])) :\n",
    "        # true value = class 1 and prediction = class 1\n",
    "        if (df[\"Id\"][i] - 1) < N_true_train and df[\"Value\"][i] == 1 :\n",
    "            tp += 1\n",
    "        # true value = class 1 and prediction = class 0\n",
    "        elif df[\"Id\"][i] - 1 < N_true_train and df[\"Value\"][i] == 0 :\n",
    "            fn += 1\n",
    "        # true value = class 0 and prediction = class 1\n",
    "        elif df[\"Id\"][i] - 1 >= N_true_train and df[\"Value\"][i] == 1 :\n",
    "            fp += 1\n",
    "        else :\n",
    "            tn += 1\n",
    "    if tp == 0 :\n",
    "        return 0, 0\n",
    "    else :\n",
    "        pre = tp / (tp + fp) # precision\n",
    "        recall = tp / (tp + fn)\n",
    "        return pre, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Training Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D : training doc\n",
    "# train_df : training dataframe\n",
    "# val : test doc\n",
    "# ind_val : test indicies\n",
    "# new_t_train : true class training doc\n",
    "# new_f_train : fake class training doc\n",
    "# final_all : features\n",
    "# typ : training type 1-> NB, 2 -> Rocchio, 3 -> kNN\n",
    "def TrainingMethod(new_train, train_df, val, ind_val, new_t_train, new_f_train, final_all, typ) :\n",
    "    doc_id = []\n",
    "    value = []\n",
    "    # training : NB\n",
    "    if typ == 1 :\n",
    "        prior, condprob = TrainNB(new_t_train, new_f_train, final_all, len(new_t_train), len(new_f_train))\n",
    "        for i in range(len(val)) :\n",
    "            doc_id.append(ind_val[i] + 1)\n",
    "            value.append(ApplyNB(val[i], prior, condprob, final_all))\n",
    "            \n",
    "    # training : Rocchio\n",
    "    elif typ == 2 :\n",
    "        train_vec = DocToVector(new_train, final_all, True)\n",
    "        test_vec = DocToVector(val, final_all, False)\n",
    "        centroid = TrainRocchio(train_vec, len(final_all), N_true)\n",
    "        for i in range(len(test_vec)):\n",
    "            doc_id.append(ind_val[i] + 1)\n",
    "            value.append(ApplyRocchio(centroid, test_vec[i]))\n",
    "            \n",
    "    # training : kNN\n",
    "    elif typ == 3 :    \n",
    "        train_vec = DocToVector(new_train, final_all, True)\n",
    "        test_vec = DocToVector(val, final_all, False)\n",
    "        train_vec_df = pd.DataFrame({'vec': train_vec, 'label': train_df['label']})\n",
    "        doc_id = []\n",
    "        value = []\n",
    "        for i in range(len(test_vec)):\n",
    "            doc_id.append(ind_val[i] + 1)\n",
    "            value.append(ApplyKNN(train_vec_df, 5, test_vec[i]))\n",
    "            \n",
    "    d = {\"Id\" : doc_id , \"Value\" : value}\n",
    "    df = pd.DataFrame(data = d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I : document indicies\n",
    "# D : training doc\n",
    "# k : # of folds\n",
    "def CrossValidation(I, D, k, N_true_train) :    \n",
    "    # Split the data into 10 folds\n",
    "    ind_fold = []\n",
    "    fold = []\n",
    "    ind_fold, fold = SplitData(I, D, k)\n",
    "    ave = 0\n",
    "    new_train = []\n",
    "    new_train.append([])\n",
    "    val = []\n",
    "    val.append([])\n",
    "    ind_new_train = []\n",
    "    ind_new_train.append([])\n",
    "    ind_val = []\n",
    "    ind_val.append([])\n",
    "    train_t = []\n",
    "    train_t.append([])\n",
    "    train_f = []\n",
    "    train_f.append([])\n",
    "    all_labels = []\n",
    "    all_labels.append([])\n",
    "    train_df = []\n",
    "    for i in range(k) :\n",
    "        temp_train = []\n",
    "        temp_val = []\n",
    "        temp_ind = []\n",
    "        \n",
    "        # val : take one fold for testing\n",
    "        ind_val[0].append(ind_fold[i])\n",
    "        val[0].append(fold[i])\n",
    "        \n",
    "        # new_train : merge the other nine folds\n",
    "        for j in range(k) :\n",
    "            if i != j :\n",
    "                temp_ind.extend(ind_fold[j])\n",
    "                temp_train.extend(fold[j])\n",
    "        new_train[0].append(temp_train)  \n",
    "        ind_new_train[0].append(temp_ind)\n",
    "    for i in range(k) :\n",
    "        temp_t, temp_f, labels = SeperateTrueFake(ind_new_train[0][i], new_train[0][i], N_true_train)\n",
    "        train_t[0].append(temp_t)\n",
    "        train_f[0].append(train_f)\n",
    "        all_labels[0].append(labels)\n",
    "        train_df.append(pd.DataFrame({'terms': new_train[0][i], 'label': labels}))\n",
    "    return new_train[0], val[0], ind_new_train[0], ind_val[0], train_t[0], train_f[0], all_labels[0], train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches of Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 300 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "fe_select = [\"Chi-square\", \"EMI\", \"LLR\"]\n",
    "training_method = [\"NB\", \"Rocchio\", \"kNN\"]\n",
    "final_all = []\n",
    "train_df = []\n",
    "train_t = []\n",
    "train_f = []\n",
    "all_labels = []\n",
    "new_train = []\n",
    "val = []\n",
    "ind_new_train = []\n",
    "ind_val = []\n",
    "new_train, val, ind_new_train, ind_val, train_t, train_f, all_labels, train_df = CrossValidation(ind_train, train, k, N_true_train)\n",
    "# feature selection\n",
    "for i in range(10) :\n",
    "    final_all.append(FeatureSelection(train_df[i], 300, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : NB\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.6370467439755327\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 1)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[0])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rocchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : Rocchio\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.4316579616572813\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 2)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[1])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : kNN\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.8625073745663484\n"
     ]
    }
   ],
   "source": [
    "F1 = 0\n",
    "maxF1 = 0\n",
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 3)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[2])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 500 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "fe_select = [\"Chi-square\", \"EMI\", \"LLR\"]\n",
    "training_method = [\"NB\", \"Rocchio\", \"kNN\"]\n",
    "final_all = []\n",
    "train_df = []\n",
    "train_t = []\n",
    "train_f = []\n",
    "all_labels = []\n",
    "new_train = []\n",
    "val = []\n",
    "ind_new_train = []\n",
    "ind_val = []\n",
    "new_train, val, ind_new_train, ind_val, train_t, train_f, all_labels, train_df = CrossValidation(ind_train, train, k, N_true_train)\n",
    "# feature selection\n",
    "for i in range(10) :\n",
    "    final_all.append(FeatureSelection(train_df[i], 500, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : NB\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.6370322236662853\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 1)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[0])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rocchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : Rocchio\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.5978647003387705\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 2)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[1])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : kNN\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.8660578405201083\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 3)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[2])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 800 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "fe_select = [\"Chi-square\", \"EMI\", \"LLR\"]\n",
    "training_method = [\"NB\", \"Rocchio\", \"kNN\"]\n",
    "final_all = []\n",
    "train_df = []\n",
    "train_t = []\n",
    "train_f = []\n",
    "all_labels = []\n",
    "new_train = []\n",
    "val = []\n",
    "ind_new_train = []\n",
    "ind_val = []\n",
    "new_train, val, ind_new_train, ind_val, train_t, train_f, all_labels, train_df = CrossValidation(ind_train, train, k, N_true_train)\n",
    "# feature selection\n",
    "for i in range(10) :\n",
    "    final_all.append(FeatureSelection(train_df[i], 800, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : NB\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.6370381079779972\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 1)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[0])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rocchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : Rocchio\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.36016670375920334\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 2)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[1])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : kNN\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.8660794511309172\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 3)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[2])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 1000 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "fe_select = [\"Chi-square\", \"EMI\", \"LLR\"]\n",
    "training_method = [\"NB\", \"Rocchio\", \"kNN\"]\n",
    "final_all = []\n",
    "train_df = []\n",
    "train_t = []\n",
    "train_f = []\n",
    "all_labels = []\n",
    "new_train = []\n",
    "val = []\n",
    "ind_new_train = []\n",
    "ind_val = []\n",
    "new_train, val, ind_new_train, ind_val, train_t, train_f, all_labels, train_df = CrossValidation(ind_train, train, k, N_true_train)\n",
    "# feature selection\n",
    "for i in range(10) :\n",
    "    final_all.append(FeatureSelection(train_df[i], 1000, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : NB\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.6370303699016749\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 1)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[0])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rocchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : Rocchio\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.5538185959806378\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 2)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[1])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Method : kNN\n",
      "Feature Selection: Chi-square\n",
      "F1 : 0.8645996522380022\n"
     ]
    }
   ],
   "source": [
    "ave = 0\n",
    "# 10-fold cross validation\n",
    "for j in range(10) :\n",
    "    df = TrainingMethod(new_train[j], train_df[j], val[j], ind_val[j], train_t[j], train_f[j], final_all[j], 3)\n",
    "    pre, recall = CountPreRecall(df, N_true_train)\n",
    "    F1 = CountF1(pre, recall) * ((len(val[j])) / len(ind_train))\n",
    "    ave += F1\n",
    "print(\"Training Method :\", training_method[2])\n",
    "print(\"Feature Selection:\", fe_select[0])\n",
    "print(\"F1 :\", ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'terms': train, 'label': Labels(train, N_true_train)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_list2 = true_test.tolist()\n",
    "fake_list2 = fake_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_true_test = len(true_list2)\n",
    "N_fake_test = len(fake_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_fake_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2545"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_true_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_token_list = lower_split(true_list)\n",
    "f_token_list = lower_split(fake_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stop_list = stopwords(t_token_list, stopwords_list)\n",
    "f_stop_list = stopwords(f_token_list, stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list_of_list = stem(t_stop_list)\n",
    "f_list_of_list = stem(f_stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test = stopwords(t_token_list, stopwords_list)\n",
    "f_test = stopwords(f_token_list, stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "test.extend(t_test)\n",
    "test.extend(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[] for i in range(len(test))]\n",
    "for i in range(len(test)):\n",
    "    if i < N_true_test:\n",
    "        labels[i] = 1\n",
    "    else:\n",
    "        labels[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'terms': test, 'label': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_all = pd.DataFrame()\n",
    "final_all = FeatureSelection(train_df, 800, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent Documents as Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = DocToVector(train, final_all, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_df = pd.DataFrame({'vec': train_vec, 'label': train_df['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = DocToVector(test, final_all, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec_df = pd.DataFrame({'vec': test_vec, 'label': test_df['label']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_performance(answer, predict):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for i in range(len(true_answer)):\n",
    "        if(predict[i] == answer[i] == 1):\n",
    "            tp = tp + 1\n",
    "        elif(predict[i] == answer[i] == 0):\n",
    "            tn = tn + 1\n",
    "        elif(predict[i] == 1 and answer[i] == 0):\n",
    "            fp = fp + 1\n",
    "        elif(predict[i] == 0 and answer[i] == 1):\n",
    "            fn = fn + 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"precision = \" + str(precision))\n",
    "    print(\"recall = \" + str(recall))\n",
    "    print(\"F1 = \" + str(F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing KNN (with k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40375/40375 [00:00<00:00, 124001.33it/s]\n"
     ]
    }
   ],
   "source": [
    "true_answer = []\n",
    "for i in tqdm(range(len(test_vec))):\n",
    "    true_answer.append(test_vec_df['label'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40375/40375 [1:01:17<00:00, 10.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "ans = []\n",
    "for i in tqdm(range(len(test_vec))):\n",
    "    ans.append(ApplyKNN(train_vec_df, 3, test_vec[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.12588664235996022\n",
      "recall = 0.7461689587426326\n",
      "F1 = 0.21542824730572885\n"
     ]
    }
   ],
   "source": [
    "testing_performance(true_answer, ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
